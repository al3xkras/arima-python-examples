{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da7e530",
   "metadata": {},
   "source": [
    "### Project 5 Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69811e5b",
   "metadata": {},
   "source": [
    "## Project 5: ARIMA Time Series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a35026c",
   "metadata": {},
   "source": [
    "#### 1. Data Source:\n",
    "- Task 1: NBP dataset, exchange rates bewteen PLN and THB\n",
    "- Task 2: cryptocurrency exchange rates between USD and BTC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b3f52",
   "metadata": {},
   "source": [
    "##### 2. Research purpose:\n",
    "\n",
    "- For both PLN-THB and USD-BTC analysis:\n",
    "    - Use the ADF-test to test the stationarity of the data\n",
    "        - Def: A stochastic process is a stationary process, if its joint PDF does not change when shifted in time (if the process does not contain unit root)\n",
    "        - If a process is stationary, its moments (e.g. mean, variance, skewness, kurtosis) do not change over time\n",
    "    - Build the ARIMA model with parameters p,d,q; \n",
    "        - If the data is stationary: set d=0 (differentiation order)\n",
    "        - If the data is not stationary, increase the differentiation order until it will be possible to reject the ADF-test H0 (or use a different I model)\n",
    "    - Find parameters p,q (AR order and moving average window), that will fit the model the best\n",
    "    - Build the ARIMA model, draw the validation plot to check goodness of fit.\n",
    "    - If the goodness of fit is not sufficient, use a different integration model (the default I model is shift, however it is possible to use different models, and set the differentiation order d=0: log, exp_decay, sample - rolling mean etc)\n",
    "    - Use the most optimal (in terms of the ADF-test and currency analysis) Integration model to fit the data\n",
    "    - Build a different ARIMA model, draw the validation plot\n",
    "    - Predict the exchange rates based on the fitted ARIMA model\n",
    "    - Draw conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7f124a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from datetime import timedelta\n",
    "register_matplotlib_converters()\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14558f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confidence level\n",
    "alpha = 0.05\n",
    "\n",
    "#The name of the dataset column to analyze\n",
    "col='Open'\n",
    "\n",
    "#Select data in range [date_min,date_max]\n",
    "date_min=pd.to_datetime(\"2020.01.01\")\n",
    "date_max=pd.to_datetime(\"2020.06.01\")\n",
    "\n",
    "#Dogecoin dataset (not used in this project)\n",
    "df_doge=\"coin_Dogecoin.csv\"\n",
    "#Bitcoin dataset\n",
    "df_btc=\"coin_Bitcoin.csv\"\n",
    "#Exchange rates dataset from the NBP\n",
    "df_currencies=\"currencies.xls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a47363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class CurrencyAnalysis, all the related methods are formalized and \n",
    "# joined in a single class\n",
    "\n",
    "class CurrencyAnalysis:\n",
    "    def __init__(self,alpha,df_name,col,date_min,date_max,test_days=20,currency_name='',rolling_mean_window=12):\n",
    "        # confidence level\n",
    "        self.alpha=alpha\n",
    "        # dataset\n",
    "        self.df_name=df_name\n",
    "        # column to analyze\n",
    "        self.col=col\n",
    "        self.currency_name=currency_name\n",
    "        # The ARIMA model\n",
    "        self.model=None\n",
    "        # ARIMA model build results (cross-validation)\n",
    "        self.results=None\n",
    "        self.date_min=date_min\n",
    "        self.date_max=date_max\n",
    "        # The test data to display is chosen in range [date_max,date_test]\n",
    "        self.date_test=date_max+timedelta(days=test_days)\n",
    "        # initialize the dataset\n",
    "        self.df=self.read_df()\n",
    "        # Parameter k of the MA model\n",
    "        self.rwindow=rolling_mean_window\n",
    "        self.test_days=test_days\n",
    "        \n",
    "    @staticmethod\n",
    "    def init_figure():\n",
    "        # resize the matplotlib figure\n",
    "        plt.gcf().set_size_inches(8,3.5)\n",
    "    \n",
    "    def _read_df(self,index='Date'):\n",
    "        # (default) read the BTC dataset (csv)\n",
    "        df1=pd.read_csv(self.df_name)\n",
    "        df1[index]=df1[index].apply(pd.to_datetime)\n",
    "        df1=df1[(df1[index]>=date_min) & (df1[index]<=date_max)]\n",
    "        df1=df1.set_index(index)\n",
    "        df1[self.col]=df1[self.col].astype(float)\n",
    "        return df1\n",
    "    \n",
    "    def read_df(self,index='Date'):\n",
    "        try:\n",
    "            return self._read_df(index)\n",
    "        except:\n",
    "            return self.parse_data2([self.col])\n",
    "        \n",
    "    def parse_data2(self,cols):\n",
    "        # read the NBP dataset (xlsx)\n",
    "        fname=self.df_name\n",
    "        df=pd.read_excel(fname)\n",
    "        df=df.iloc[1:237]\n",
    "        df['Date']=df['data'].apply(pd.to_datetime)\n",
    "        for x in cols:\n",
    "            df[x]=df[x].astype(float)\n",
    "        df['Open']=df[cols[0]]\n",
    "        df=df[(df['Date']>=self.date_min) & (df['Date']<=self.date_max)]\n",
    "        return df\n",
    "    \n",
    "    def _test(self,col='Open',index='Date',limits=None):\n",
    "        # read the test dataset 1 (BTC-USD)\n",
    "        df_test=pd.read_csv(self.df_name)\n",
    "        df_test[index]=df_test[index].apply(pd.to_datetime)\n",
    "        if limits is None:\n",
    "            df_test=df_test[(df_test[index]>=self.date_max) & (df_test[index]<=self.date_test)]\n",
    "        else:\n",
    "            df_test=df_test[(df_test[index]>=limits[0]) & (df_test[index]<=limits[1])]\n",
    "        df_test=df_test.set_index(index)\n",
    "        df_test=df_test[self.col].astype(float)\n",
    "        return df_test\n",
    "    \n",
    "    def test(self,col='Open',index='Date',limits=None):\n",
    "        try:\n",
    "            return self._test(col,index,limits)\n",
    "        except:\n",
    "            # read the test dataset 2 (PLN-THB)\n",
    "            fname=self.df_name\n",
    "            df=pd.read_excel(fname)\n",
    "            df=df.iloc[1:237]\n",
    "            df['Date']=df['data'].apply(pd.to_datetime)\n",
    "            df[self.col]=df[self.col].astype(float)\n",
    "            df['Open']=df[self.col]\n",
    "            if limits is None:\n",
    "                df=df[(df['Date']>=self.date_max) & (df['Date']<=self.date_test)]\n",
    "            else:\n",
    "                df=df[(df['Date']>=limits[0]) & (df['Date']<=limits[1])]\n",
    "            \n",
    "            return df['Open']\n",
    "        \n",
    "    def adf_test(self,df=None,col='Open'):\n",
    "        # Implementation of the ADF-test\n",
    "        if df is None:\n",
    "            df=self.df[col]\n",
    "        result = adfuller(df)\n",
    "        print(col)\n",
    "        print('ADF Statistic: {}'.format(result[0]))\n",
    "        print('p-value: {}'.format(result[1]))\n",
    "        print('Critical Values:')\n",
    "        for key, value in result[4].items():\n",
    "            print('\\t{}: {}'.format(key, value))\n",
    "    \n",
    "    # Plot the rolling mean and std.\n",
    "    def get_stationarity(self,timeseries,title=''):\n",
    "        # rolling statistics (moving average and moving std.)\n",
    "        rolling_mean = timeseries.rolling(window=self.rwindow).mean()\n",
    "        rolling_std = timeseries.rolling(window=self.rwindow).std()\n",
    "        # rolling statistics plot\n",
    "        original = plt.plot(timeseries, color='blue', label='Original')\n",
    "        mean = plt.plot(rolling_mean, color='red', label='Rolling Mean')\n",
    "        std = plt.plot(rolling_std, color='black', label='Rolling Std')\n",
    "        plt.legend(loc='best',prop={'size': 6})\n",
    "        plt.title(title)\n",
    "        plt.show(block=False)\n",
    "\n",
    "    # Plot the rolling mean and std. && display the ADF-test results\n",
    "    def stationarity(self,adf_test=True):\n",
    "        self.init_figure()\n",
    "        self.get_stationarity(self.df[self.col],self.currency_name)\n",
    "        # Dickeyâ€“Fuller test:\n",
    "        if adf_test:\n",
    "            self.adf_test()\n",
    "    \n",
    "    # Integration model: df -> df - rolling mean\n",
    "    def df_log_minus_mean(self,plot=True):\n",
    "        df_log = self.df[self.col]\n",
    "        rolling_mean = df_log.rolling(window=12).mean()\n",
    "        df_log_minus_mean = df_log - rolling_mean\n",
    "        df_log_minus_mean.dropna(inplace=True)\n",
    "        if plot:\n",
    "            self.init_figure()\n",
    "            self.get_stationarity(df_log_minus_mean,self.currency_name)\n",
    "            self.adf_test(df_log_minus_mean)\n",
    "        return df_log_minus_mean\n",
    "    \n",
    "    # Integration model: df -> exp(df) with parameters half_life, min_periods\n",
    "    def df_log_exp_decay(self,half_life,plot=True):\n",
    "        df_log=self.df[self.col]\n",
    "        rolling_mean_exp_decay = df_log.ewm(halflife=half_life, min_periods=0, adjust=True).mean()\n",
    "        df_log_exp_decay = df_log - rolling_mean_exp_decay\n",
    "        df_log_exp_decay.dropna(inplace=True)\n",
    "        if plot:\n",
    "            self.init_figure()\n",
    "            self.get_stationarity(df_log_exp_decay,self.currency_name)\n",
    "            self.adf_test(df_log_exp_decay)\n",
    "        return df_log_exp_decay\n",
    "    \n",
    "    # Integration model: df -> df - df.shift()\n",
    "    def df_log_shift(self,plot=True):\n",
    "        df_log = self.df[self.col]\n",
    "        df_log_shift = df_log - df_log.shift()\n",
    "        df_log_shift.dropna(inplace=True)\n",
    "        if plot:\n",
    "            self.init_figure()\n",
    "            self.get_stationarity(df_log_shift,self.currency_name)\n",
    "            self.adf_test(df_log_shift)\n",
    "        return df_log_shift\n",
    "    \n",
    "    # Build the ARIMA model with parameters p (autoregressive terms), \n",
    "    # d (differentiation order), q (moving average window)\n",
    "    def arima_build(self,p,d,q,plot=True,limits=None):\n",
    "        plt.gcf().set_size_inches(4,2.5)\n",
    "        plt.gca().axes.get_xaxis().set_visible(False)\n",
    "        df_log=self.df[self.col]\n",
    "        if limits is not None:\n",
    "            df_log=df_log[limits[0]:limits[1]]\n",
    "        #decomposition = seasonal_decompose(df_log)\n",
    "        self.model = ARIMA(df_log, order=(p,d,q))\n",
    "        self.results = self.model.fit()\n",
    "        if plot:\n",
    "            # shift is the default Integration model\n",
    "            plt.plot(self.df_log_shift(plot=False))\n",
    "            plt.plot(self.results.fittedvalues, color='red')\n",
    "    \n",
    "    # Predict values based on built ARIMA model\n",
    "    # ME - mean error between predicted and expected values\n",
    "    def arima_predict(self,display_me=True):\n",
    "        results=self.results\n",
    "        df_log=self.df[self.col]\n",
    "        predictions_ARIMA_diff = pd.Series(results.fittedvalues, copy=True)\n",
    "        predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\n",
    "        predictions_ARIMA_log = pd.Series(df_log.iloc[0], index=df_log.index)\n",
    "        predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum, fill_value=0)\n",
    "        predictions_ARIMA = np.exp(predictions_ARIMA_log)\n",
    "        #self.init_figure()\n",
    "        actual=self.test('Open')\n",
    "        #plt.show()\n",
    "        predicted=results.predict(1,len(df_log)+self.test_days)\n",
    "        if isinstance(predicted.index,pd.RangeIndex):\n",
    "            predicted.index=predicted.index.map(int)\n",
    "            actual.index=actual.index.map(int)\n",
    "        predicted=predicted[len(df_log):(len(df_log)+len(actual))]\n",
    "        plt.plot(predicted,label='Predicted')\n",
    "        plt.plot(actual,label='Actual')\n",
    "        if display_me:\n",
    "            self.calc_me(actual,predicted,skip=0)\n",
    "        \n",
    "    #This function is experimental and is not unused in the project\n",
    "    def predict_iter(self,model_params,window=30,delta=1,lim=np.inf):\n",
    "        iter_date1=self.date_min\n",
    "        iter_date2=self.date_min+timedelta(window)\n",
    "        \n",
    "        me_list=[]\n",
    "        predicted_arr=[]\n",
    "        actual_arr=[]\n",
    "        i=-1\n",
    "        while(iter_date2<self.date_max and i<lim-1):\n",
    "            i+=1\n",
    "            iter_date1+=timedelta(delta)\n",
    "            iter_date2+=timedelta(delta)\n",
    "            p,d,q=model_params\n",
    "            self.arima_build(p,d,q,plot=False,limits=[iter_date1,iter_date2])\n",
    "            actual=self.test('Open',limits=[iter_date2,iter_date2+timedelta(delta+1)])\n",
    "            \n",
    "            if isinstance(actual.index,pd.DatetimeIndex):\n",
    "                actual.index=list(range(len(actual.index)))\n",
    "            \n",
    "            if len(actual)==0:\n",
    "                continue\n",
    "            predicted=self.results.predict(0,actual.index[0]+delta+1)\n",
    "            if isinstance(actual.index,pd.RangeIndex):\n",
    "                predicted.index=predicted.index.map(int)\n",
    "                predicted=predicted[(actual.index[0]-1):(actual.index[0]+delta)]\n",
    "                \n",
    "            if len(predicted)==0 or len(actual)==0:\n",
    "                print(iter_date1)\n",
    "                print(iter_date2+timedelta(delta))\n",
    "                continue\n",
    "            try:\n",
    "                me=self.calc_me(actual,predicted,skip=0,display=False)\n",
    "                me_list.append(me)\n",
    "            except:\n",
    "                print(\"AAAA\",actual,predicted)\n",
    "                \n",
    "            \n",
    "            predicted_arr.append(predicted.mean())\n",
    "            actual_arr.append(actual.mean())\n",
    "        plt.clf()\n",
    "        plt.plot(predicted_arr,label='Predicted',color='blue')\n",
    "        plt.plot(actual_arr,label='Actual',color='orange')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        return np.array(me_list)\n",
    "    \n",
    "    # Calculate the ME (mean error) between expected and actual values\n",
    "    def calc_me(self,actual,predicted,skip=0,display=True):\n",
    "        me=0\n",
    "        skipped=0\n",
    "        if isinstance(actual.index,pd.Int64Index):\n",
    "            for i in actual.index:\n",
    "                if skipped<skip:\n",
    "                    skipped+=1\n",
    "                    continue\n",
    "                me+=(actual[i]-predicted[i])\n",
    "        else:\n",
    "            for i in range(len(actual)):\n",
    "                if skipped<skip:\n",
    "                    skipped+=1\n",
    "                    continue\n",
    "                me+=(actual[i]-predicted[i])  \n",
    "        me/=len(actual-skipped)\n",
    "        if display:\n",
    "            print(\"The ME between the currency actual and predicted values is:\",me)\n",
    "        return me\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c2360f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
